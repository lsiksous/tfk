ðŸ’ˆ Introduction aux rÃ©seaux de neurones artificiels
#+PROPERTY: header-args:jupyter-python :session *Py* :results raw drawer :cache no :async yes :exports results :eval yes

#+SUBTITLE: Entrainement du modÃ¨le
#+AUTHOR: Laurent Siksous
#+EMAIL: siksous@gmail.com
# #+DATE:
#+DESCRIPTION: 
#+KEYWORDS: 
#+LANGUAGE:  fr

# specifying the beamer startup gives access to a number of
# keybindings which make configuring individual slides and components
# of slides easier.  See, for instance, C-c C-b on a frame headline.
#+STARTUP: beamer

#+STARTUP: oddeven

# we tell the exporter to use a specific LaTeX document class, as
# defined in org-latex-classes.  By default, this does not include a
# beamer entry so this needs to be defined in your configuration (see
# the tutorial).
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [bigger] 

#+LATEX_HEADER: \usepackage{listings}

#+LATEX_HEADER: \definecolor{UBCblue}{rgb}{0.04706, 0.13725, 0.26667} % UBC Blue (primary)
#+LATEX_HEADER: \usecolortheme[named=UBCblue]{structure}

# Beamer supports alternate themes.  Choose your favourite here
#+BEAMER_COLOR_THEME: dolphin
#+BEAMER_FONT_THEME:  default
#+BEAMER_INNER_THEME: [shadow]rounded
#+BEAMER_OUTER_THEME: infolines

# the beamer exporter expects to be told which level of headlines
# defines the frames.  We use the first level headlines for sections
# and the second (hence H:2) for frames.
#+OPTIONS: ^:nil H:2 toc:nil

# the following allow us to selectively choose headlines to export or not
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

# for a column view of options and configurations for the individual
# frames
#+COLUMNS: %20ITEM %13BEAMER_env(Env) %6BEAMER_envargs(Args) %4BEAMER_col(Col) %7BEAMER_extra(Extra)

# #+BEAMER_HEADER: \usebackgroundtemplate{\includegraphics[width=\paperwidth,height=\paperheight,opacity=.01]{img/bg2.jpeg}}
# #+BEAMER_HEADER: \logo{\includegraphics[height=.5cm,keepaspectratio]{img/bti_logo2.png}\vspace{240pt}}
# #+BEAMER_HEADER: \setbeamertemplate{background canvas}{\begin{tikzpicture}\node[opacity=.1]{\includegraphics [width=\paperwidth,height=\paperheight]{img/background.jpg}};\end{tikzpicture}}
# #+BEAMER_HEADER: \logo{\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{img/background.jpg}}
#+BEAMER_HEADER: \titlegraphic{\includegraphics[width=50]{img/logo.png}}
# #+BEAMER_HEADER: \definecolor{ft}{RGB}{255, 241, 229}
#+BEAMER_HEADER: \setbeamercolor{background canvas}{bg=ft}

* Preamble
** Emacs Setup                                                    :noexport:

#+begin_src emacs-lisp
(setq org-src-fontify-natively t)
#+end_src

#+RESULTS:
: t

** Imports

#+begin_src jupyter-python
%matplotlib inline
%load_ext autoreload
%autoreload 2

import os
from cycler import cycler
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from kerastuner import HyperModel, BayesianOptimization

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Normalization, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint
from keras.datasets import boston_housing

print(f"TensorFlow Version: {tf.__version__}")
#+end_src

#+RESULTS:
:results:
# Out[122]:
:end:

** Functions

#+begin_src jupyter-python
# Display all
def display_all(df):
    with pd.option_context("display.max_rows", 100, "display.max_columns", 100): 
        display(df)
#+end_src

#+RESULTS:
:results:
# Out[25]:
:end:

** Org                                                            :noexport:

#+begin_src jupyter-python
# Org-mode table formatter
import IPython
import tabulate

class OrgFormatter(IPython.core.formatters.BaseFormatter):
    format_type = IPython.core.formatters.Unicode('text/org')
    print_method = IPython.core.formatters.ObjectName('_repr_org_')

def pd_dataframe_to_org(df):
    return tabulate.tabulate(df, headers='keys', tablefmt='orgtbl', showindex='always')

ip = get_ipython()
ip.display_formatter.formatters['text/org'] = OrgFormatter()

f = ip.display_formatter.formatters['text/org']
f.for_type_by_name('pandas.core.frame', 'DataFrame', pd_dataframe_to_org)
#+end_src

#+RESULTS:
:results:
# Out[26]:
:end:

* Data Loading
** Load Data

#+begin_src jupyter-python
(x_train, y_train), (x_test, y_test) = boston_housing.load_data(test_split=0.2, seed=0)
x_train.shape, x_test.shape
#+end_src

#+RESULTS:
:results:
# Out[14]:
: ((404, 13), (102, 13))
:end:

** Feature Scaling

#+begin_src jupyter-python
normalizer = Normalization(axis=-1)
normalizer.adapt(np.array(x_train))
#+end_src

#+RESULTS:
:results:
# Out[17]:
:end:


#+begin_src jupyter-python
print(normalizer.mean.numpy())
#+end_src

#+RESULTS:
:results:
# Out[18]:
:end:


* Baseline

#+begin_src jupyter-python
def build_and_compile_model(norm):
  model = Sequential([
      norm,
      Dense(8, activation='relu', input_shape=(x_train.shape[1],)),
      Dense(16, activation='relu'),
      Dropout(0.1),
      Dense(1)
  ])

  model.compile(loss='mean_squared_error',
                metrics=['mse'],
                optimizer='rmsprop')
  return model
#+end_src

#+RESULTS:
:results:
# Out[37]:
:end:

#+begin_src jupyter-python
model = build_and_compile_model(normalizer)
model.summary()
#+end_src

#+RESULTS:
:results:
# Out[38]:
:end:


#+begin_src jupyter-python
%%time
history = model.fit(
    x_train,
    y_train,
    validation_split=0.2,
    verbose=0, epochs=100)
#+end_src

#+RESULTS:
:results:
# Out[32]:
:end:

#+begin_src jupyter-python
def plot_loss(history):
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'], label='val_loss')
  plt.ylim([0, 10])
  plt.xlabel('Epoch')
  plt.ylabel('Error [MEDV]')
  plt.legend()
  plt.grid(True)
  
plot_loss(history)
#+end_src

#+RESULTS:
:results:
# Out[33]:
[[file:./obipy-resources/W7H8r8.png]]
:end:

* Tuning Hyperparameters 

#+begin_src jupyter-python
class RegressionHyperModel(HyperModel):
    def __init__(self, input_shape, norm):
        self.input_shape = input_shape
        self.norm = norm
        
    def build(self, hp):
        model = Sequential()

        model.add(
            self.norm
        )

        model.add(
            Dense(
                units=hp.Int('units_1', 8, 64, 4, default=8),
                activation=hp.Choice(
                    'dense_activation_1',
                    values=['relu', 'tanh', 'sigmoid'],
                    default='relu'),
                input_shape=self.input_shape
            )
        )
        
        model.add(
            Dense(
                units=hp.Int('units_2', 16, 64, 4, default=16),
                activation=hp.Choice(
                    'dense_activation_2',
                    values=['relu', 'tanh', 'sigmoid'],
                    default='relu')
            )
        )
        
        model.add(
            Dropout(
                hp.Float(
                    'dropout',
                    min_value=0.0,
                    max_value=0.1,
                    default=0.005,
                    step=0.01)
            )
        )
        
        model.add(Dense(1))
        
        model.compile(
            optimizer='rmsprop',loss='mse',metrics=['mse']
        )
        
        return model
#+end_src

#+RESULTS:
:results:
# Out[87]:
:end:

#+begin_src jupyter-python
input_shape = (x_train.shape[1],)
hypermodel = RegressionHyperModel(input_shape, normalizer)
#+end_src

#+RESULTS:
:results:
# Out[106]:
:end:

#+begin_src jupyter-python
checkpoint_path = "ckpt/"
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = ModelCheckpoint(filepath=checkpoint_path,
                              save_weights_only=True,
                              verbose=2)
#+end_src

#+RESULTS:
:results:
# Out[107]:
:end:

#+begin_src jupyter-python
%%time
tuner_bo = BayesianOptimization(
    hypermodel,
    objective='mse',
    max_trials=10,
    seed=0,
    executions_per_trial=2,
    callbacks=[cp_callback]
)

tuner_bo.search(x_train, y_train, epochs=10, validation_split=0.2, verbose=0)
best_model = tuner_bo.get_best_models(num_models=1)[0]
#+end_src

#+RESULTS:
:results:
# Out[108]:
:end:

#+begin_src jupyter-python
best_model.summary()
#+end_src

#+RESULTS:
:results:
# Out[115]:
:end:

#+begin_src jupyter-python
# Get the optimal hyperparameters
best_hps=tuner_bo.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is {best_hps.get('units_1')} and the optimal activation is {best_hps.get('dense_activation_1')}.
The optimal number of units in the second densely-connected
layer is {best_hps.get('units_2')} and the optimal activation is {best_hps.get('dense_activation_2')}.
And the dropout is {best_hps.get('dropout')}
""")
#+end_src

#+RESULTS:
:results:
# Out[103]:
:end:


#+begin_src jupyter-python
best_model.fit(
    x_train, 
    y_train,
    epochs=20,
    batch_size=64
)
#+end_src

#+RESULTS:
:results:
# Out[118]:
: <keras.callbacks.History at 0x150ee0e20>
:end:

* Evaluation

#+begin_src jupyter-python
best_model.evaluate(x_train, y_train)
#+end_src

#+RESULTS:
:results:
# Out[119]:
: [25.440704345703125, 25.440704345703125]
:end:

* Save best model

#+begin_src jupyter-python
best_model.save('data/model')
#+end_src

#+RESULTS:
:results:
# Out[109]:
:end:

* Bibliography
** References
:PROPERTIES:
:BEAMER_opt: shrink=10
:END:

bibliographystyle:unsrt
bibliography:perceptrons.bib

* Local Variables                                                  :noexport:
# Local Variables:
# eval: (setenv "PATH" "/Library/TeX/texbin/:$PATH" t)
# org-ref-default-bibliography: ("./olist.bib")
# End:
