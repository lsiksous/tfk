ðŸ’ˆ Introduction aux rÃ©seaux de neurones artificiels
#+PROPERTY: header-args:jupyter-python :session *Py* :results raw drawer :cache no :async yes :exports results :eval yes

#+SUBTITLE: Entrainement du modÃ¨le
#+AUTHOR: Laurent Siksous
#+EMAIL: siksous@gmail.com
# #+DATE:
#+DESCRIPTION: 
#+KEYWORDS: 
#+LANGUAGE:  fr

# specifying the beamer startup gives access to a number of
# keybindings which make configuring individual slides and components
# of slides easier.  See, for instance, C-c C-b on a frame headline.
#+STARTUP: beamer

#+STARTUP: oddeven

# we tell the exporter to use a specific LaTeX document class, as
# defined in org-latex-classes.  By default, this does not include a
# beamer entry so this needs to be defined in your configuration (see
# the tutorial).
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [bigger] 

#+LATEX_HEADER: \usepackage{listings}

#+LATEX_HEADER: \definecolor{UBCblue}{rgb}{0.04706, 0.13725, 0.26667} % UBC Blue (primary)
#+LATEX_HEADER: \usecolortheme[named=UBCblue]{structure}

# Beamer supports alternate themes.  Choose your favourite here
#+BEAMER_COLOR_THEME: dolphin
#+BEAMER_FONT_THEME:  default
#+BEAMER_INNER_THEME: [shadow]rounded
#+BEAMER_OUTER_THEME: infolines

# the beamer exporter expects to be told which level of headlines
# defines the frames.  We use the first level headlines for sections
# and the second (hence H:2) for frames.
#+OPTIONS: ^:nil H:2 toc:nil

# the following allow us to selectively choose headlines to export or not
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

# for a column view of options and configurations for the individual
# frames
#+COLUMNS: %20ITEM %13BEAMER_env(Env) %6BEAMER_envargs(Args) %4BEAMER_col(Col) %7BEAMER_extra(Extra)

# #+BEAMER_HEADER: \usebackgroundtemplate{\includegraphics[width=\paperwidth,height=\paperheight,opacity=.01]{img/bg2.jpeg}}
# #+BEAMER_HEADER: \logo{\includegraphics[height=.5cm,keepaspectratio]{img/bti_logo2.png}\vspace{240pt}}
# #+BEAMER_HEADER: \setbeamertemplate{background canvas}{\begin{tikzpicture}\node[opacity=.1]{\includegraphics [width=\paperwidth,height=\paperheight]{img/background.jpg}};\end{tikzpicture}}
# #+BEAMER_HEADER: \logo{\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{img/background.jpg}}
#+BEAMER_HEADER: \titlegraphic{\includegraphics[width=50]{img/logo.png}}
# #+BEAMER_HEADER: \definecolor{ft}{RGB}{255, 241, 229}
#+BEAMER_HEADER: \setbeamercolor{background canvas}{bg=ft}

* Preamble
** Emacs Setup                                                    :noexport:

#+begin_src emacs-lisp
(setq org-src-fontify-natively t)
#+end_src

#+RESULTS:
: t

** Imports

#+begin_src jupyter-python
%matplotlib inline
%load_ext autoreload
%autoreload 2

from seaborn.relational import lineplot
import os
from cycler import cycler
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from kerastuner import HyperModel, BayesianOptimization

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Normalization, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

print(f"TensorFlow Version: {tf.__version__}")
#+end_src

#+RESULTS:
:results:
# Out[70]:
:end:

** Functions

#+begin_src jupyter-python
# Display all
def display_all(df):
    with pd.option_context("display.max_rows", 100, "display.max_columns", 20): 
        display(df)
#+end_src

#+RESULTS:
:results:
# Out[71]:
:end:

** Org                                                            :noexport:

#+begin_src jupyter-python
# Org-mode table formatter
import IPython
import tabulate

class OrgFormatter(IPython.core.formatters.BaseFormatter):
    format_type = IPython.core.formatters.Unicode('text/org')
    print_method = IPython.core.formatters.ObjectName('_repr_org_')

def pd_dataframe_to_org(df):
    return tabulate.tabulate(df, headers='keys', tablefmt='orgtbl', showindex='always')

ip = get_ipython()
ip.display_formatter.formatters['text/org'] = OrgFormatter()

f = ip.display_formatter.formatters['text/org']
f.for_type_by_name('pandas.core.frame', 'DataFrame', pd_dataframe_to_org)
#+end_src

#+RESULTS:
:results:
# Out[72]:
:end:

** Seaborn

#+begin_src jupyter-python
sns.set(rc={'figure.figsize':(11.7,8.27)})
#+end_src

#+RESULTS:
:results:
# Out[73]:
:end:


* Load Data

#+begin_src jupyter-python
X = load_breast_cancer()['data']
y = load_breast_cancer()['target']
feature_names = load_breast_cancer()['feature_names']
df = pd.DataFrame(X, columns=feature_names)
#+end_src

#+RESULTS:
:results:
# Out[74]:
:end:


* Split the data

#+begin_src jupyter-python
x_train, x_test, y_train, y_test = train_test_split(X, y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=42)
x_train.shape, y_train.shape
#+end_src

#+RESULTS:
:results:
# Out[75]:
: ((455, 30), (455,))
:end:

** Feature Scaling

#+begin_src jupyter-python
normalizer = Normalization(axis=-1)
normalizer.adapt(np.array(x_train))
#+end_src

#+RESULTS:
:results:
# Out[76]:
:end:


#+begin_src jupyter-python
print(normalizer.mean.numpy())
#+end_src

#+RESULTS:
:results:
# Out[77]:
:end:

* Tuning Hyperparameters 

#+begin_src jupyter-python
class ClassificationHyperModel(HyperModel):
    def __init__(self, input_shape, norm):
        self.input_shape = input_shape
        self.norm = norm
        
    def build(self, hp):
        model = Sequential()

        model.add(
            self.norm
        )

        model.add(
            Dense(
                units=hp.Int('units_1', 8, 64, 4, default=8),
                activation=hp.Choice(
                    'dense_activation_1',
                    values=['relu', 'tanh', 'sigmoid'],
                    default='relu'),
                input_shape=self.input_shape
            )
        )
        
        model.add(
            Dense(
                units=hp.Int('units_2', 16, 64, 4, default=16),
                activation=hp.Choice(
                    'dense_activation_2',
                    values=['relu', 'tanh', 'sigmoid'],
                    default='relu')
            )
        )
        
        model.add(
            Dropout(
                hp.Float(
                    'dropout',
                    min_value=0.0,
                    max_value=0.1,
                    default=0.005,
                    step=0.01)
            )
        )
        
        model.add(Dense(2, activation='softmax'))

        lr = hp.Choice('learning_rate',
            values=[1e-1, 1e-2, 1e-3])
        opt = Adam(learning_rate=lr)
        
        model.compile(
            optimizer=opt,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
#+end_src

#+RESULTS:
:results:
# Out[78]:
:end:

#+begin_src jupyter-python
input_shape = (x_train.shape[1],)
hypermodel = ClassificationHyperModel(input_shape, normalizer)
#+end_src

#+RESULTS:
:results:
# Out[79]:
:end:

#+begin_src jupyter-python
%%time
tuner_bo = BayesianOptimization(
    hypermodel,
    objective='accuracy',
    max_trials=10,
    seed=42,
    executions_per_trial=2,
    directory='model/kt',
    project_name='bcw'

)

tuner_bo.search(x_train, y_train, epochs=10, validation_split=0.2, verbose=0)
best_model = tuner_bo.get_best_models(num_models=1)[0]
#+end_src

#+RESULTS:
:results:
# Out[80]:
:end:

#+begin_src jupyter-python :results output
best_model.summary()
#+end_src

#+RESULTS:
:results:
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 normalization (Normalizatio  (None, 30)               61        
 n)                                                              
                                                                 
 dense (Dense)               (None, 44)                1364      
                                                                 
 dense_1 (Dense)             (None, 36)                1620      
                                                                 
 dropout (Dropout)           (None, 36)                0         
                                                                 
 dense_2 (Dense)             (None, 2)                 74        
                                                                 
=================================================================
Total params: 3,119
Trainable params: 3,058
Non-trainable params: 61
_________________________________________________________________
:end:

#+begin_src jupyter-python :results output
# Get the optimal hyperparameters
best_hps=tuner_bo.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is {best_hps.get('units_1')} and the optimal activation is {best_hps.get('dense_activation_1')}.
The optimal number of units in the second densely-connected
layer is {best_hps.get('units_2')} and the optimal activation is {best_hps.get('dense_activation_2')}.
The learning rate is {best_hps.get('learning_rate')}.
And the dropout is {best_hps.get('dropout')}
""")
#+end_src

#+RESULTS:
:results:

The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is 44 and the optimal activation is relu.
The optimal number of units in the second densely-connected
layer is 36 and the optimal activation is tanh.
The learning rate is 0.1.
And the dropout is 0.09

:end:

- Finally we fit our best model:

#+begin_src jupyter-python
checkpoint_path = "model/bcw/checkpoint"
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = ModelCheckpoint(filepath=checkpoint_path,
                              monitor='val_loss',
                              mode='min',
                              save_weights_only=True,
                              save_freq=50,
                              save_best_only=True,
                              verbose=1)

es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

history = best_model.fit(x_train, 
                         y_train,
                         epochs=20,
                         batch_size=len(y_train),
                         validation_split=0.2,
                         callbacks=[cp_callback, es_callback]
)
#+end_src

#+RESULTS:
:results:
# Out[83]:
:end:


* Evaluation

#+begin_src jupyter-python
val_loss, val_accuracy = best_model.evaluate(x_train, y_train)
val_loss, val_accuracy
#+end_src

#+RESULTS:
:results:
# Out[84]:
: (0.058622051030397415, 0.9868132472038269)
:end:

#+begin_src jupyter-python
def plot_fit(history, metric):
  plt.plot(history.history[f'{metric}'], label=f'{metric}')
  plt.plot(history.history[f'val_{metric}'], label=f'val_{metric}')
  plt.xlabel('Epoch')
  plt.ylabel('Error [MEDV]')
  plt.legend()
  plt.grid(True)
  
plot_fit(history, 'loss')
#+end_src

#+RESULTS:
:results:
# Out[85]:
[[file:./obipy-resources/Ef1wPI.png]]
:end:



* Save best model

#+begin_src jupyter-python
best_model.save('model/bcw')
#+end_src

#+RESULTS:
:results:
# Out[86]:
:end:

* Bibliography
** References
:PROPERTIES:
:BEAMER_opt: shrink=10
:END:

bibliographystyle:unsrt
bibliography:tfk.bib

* Local Variables                                                  :noexport:
# Local Variables:
# eval: (setenv "PATH" "/Library/TeX/texbin/:$PATH" t)
# org-ref-default-bibliography: ("./olist.bib")
# End:
